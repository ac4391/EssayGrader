{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook is for the purpose of converting the essays in string form to vectorized form. This notebook makes use of the GLoVe word embeddings. More information about these word embeddings can be found here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "If you'd like to move directly to the Neural Network, you can skip this notebook completely and load the prepared data directly!\n",
    "\n",
    "If you would like to follow along with the preprocessing work, follow the link above and download the glove.6b.zip file to the \"data/\" directory of this project. Then uncompress the zip file to reveal directory named \"glove.6B/\" with 4 txt files inside. These are the four available word embeddings. You are not able to proceed with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.preprocess import get_data, vectorize_essays, normalize_scores\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 3,
   "source": [
    "Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data from tsv file and store in a pandas dataframe\n",
    "data_path = './data/training_set_rel3.tsv'\n",
    "essay_df = get_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contents of the dataframe to understand what information is provided in the training set\n",
    "More EDA if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of essays is 10684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 Essays Vectorized\n5000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 Essays Vectorized\n7000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Essays Vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10684 Total Essays Vectorized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/EssayGrader_env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "essay_df = essay_df[essay_df['essay_set']<=6]\n",
    "essays = essay_df['essay']\n",
    "essay_df['essays_embed'] = vectorize_essays(essays, 100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in range(1,7):\n",
    "    if set==2:\n",
    "        essay_df[essay_df['essay_set']]==set\n",
    "    else:\n",
    "        scores = essay_df[essay_df['essay_set']==set]['domain1_score']\n",
    "        maximum = max(scores)\n",
    "        norm_scores = normalize_scores(scores, maximum)\n",
    "\n",
    "#essay_df['norm_scores'] = normalize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n       'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3',\n       'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1',\n       'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5',\n       'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3',\n       'rater3_trait4', 'rater3_trait5', 'rater3_trait6', 'essays_embed'],\n      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_df[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the preprocessed data frame for later use\n",
    "essay_df.to_pickle('./data/essay_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}